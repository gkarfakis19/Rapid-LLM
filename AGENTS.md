RAPID-LLM quick brief for future agents

- Purpose: Python simulator for LSTM/GEMM/LLM performance, memory, and (rudimentary) energy. Entry point `run_perf.py` loads YAML configs (hardware + model) and builds compute/comm graphs, then simulates analytically or via AstraSim.
- Execution backends (set in hardware config `execution_backend`): analytical (ring-only, no congestion); hybrid (AstraSim for transformer block graph, analytical pipeline); full_astrasim_hierarchical (AstraSim separately for transformer and pipeline graphs, assumes no congestion between them); full_astrasim_flattened (one big combined graph in AstraSim, slowest/most detailed).
- Parallelism model: supports DP, TP (+optional sequence parallel), CP, GPipe-style pipeline (`pp` stages), and hybrid TP+CP. Pipeline graph assumes uniform per-layer timing and GPipe scheduling; gradient accumulation builds a no-DP variant so only the last accumulation step does DP all-reduce.
- Communication assumptions: blocking collectives; analytical path uses ring formulas; AstraSim collectives flatten 2D topologies to 1D for collectives. Hierarchical mode requires the first active network dimension map exactly to {tp,cp}; non-ring topologies are rejected in analytical mode. Faulty links only valid in full AstraSim modes.
- ZeRO assumptions: stage 0/1 â‰ˆ DDP; stage 2 adds per-layer param AG + grad RS sizes; stage 3 adds per-layer param materialization (counted as comm + ephemeral memory, no overlap). MoE assumes perfect balance and TP all-to-all; multi-GPU MoE is WIP.
- Attention: dense by default. FlashAttention supported for training forward/prefill when `attention.use_flashattention=true` and tile size is set manually; decode path ignores FlashAttention. Sliding-window/MLA are WIP.
- Memory model: static formulas for activations/weights/optimizer; KV-cache memory estimation WIP. Peak memory compared to DRAM per device; ZeRO-3 ephemeral gather tracked separately. No fragmentation/eviction modeling.
- Inference: `TimeCalculationLLMInference` handles prefill + sampled decode steps (linear interpolation between samples). DP acts as replica multiplier only; CP decode is noted WIP. FlashAttention disabled for decode.
- Artifacts/logs: outputs under `output/` (mode subdir). AstraSim artifacts live in `astra_cache/` and optionally `output/LLM/astra_hier` or `astra_flat` when persistence flags are set. Current logs/marker: `log/this_folder_contains_astrasim_logs`, `log/log.log` hold prior runs. Debug hardware snapshots in `dbg_hw_conf/20251105_173209/` (e.g., `baseline_001_tp64_cp1_dp1_lp1.yaml`, `baseline_053_tp1_cp1_dp64_lp1.yaml`).
- Key environment flags: `RAPID_VISUALIZE_GRAPHS`, `RAPID_PERSIST_ASTRASIM_ARTIFACTS`, `RAPID_PERSIST_ARTIFACT_VIZ`, `RAPID_ASTRA_CACHE_MODE` (default CACHE_READWRITE).


Use the python interpreter in $REPO_ROOT$/venv/bin/python to run the scripts, not the system python/python3 binaries.