model_param:
  mode: "LLM"
  run_type: "inference"
  tied_embeddings: true
  model_type: "llama"  # gpt assumes dense-style architecture with GELU; llama assumes llama-style architecture with SwiGLU
  batch_size: 16
  seq_len: 2048        # 32768 prefill + 1024 decode
  decode_len: 1024
  # prefill len = seq_len - decode_len
  hidden_dim: 4096
  attention:
    attention_type: "mha"  # Supported attention types: "mha" or "gqa" ("mla" not implemented)
    num_heads: 32
    kv_heads: 8  # set only when attention_type is "gqa"
    use_flashattention: true  # Whether to use flash attention
    attention_tile_size: 256  # Tile size for attention computation
  num_experts: 16  # Number of experts in MoE layers. Set to 1 to disable MoE.
  top_k: 2  # Top-k experts to use in MoE layers
  ffn_dim: 11008
  ffn_mult: null
  vocab_size: 32000
  num_layers: 32
inference_param:
  sample_every: -1
# No GQA; KV heads = 32. FlashDecoding++ BS=1, input=32K reference.
