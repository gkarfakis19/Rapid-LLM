model_param:
    mode: "LLM"  # Mode of operation
    run_type: "training"
    tied_embeddings: false  # Share input and output embedding weights
    model_type: "gpt"  # gpt assumes dense-style architecture with GELU; llama assumes llama-style architecture with SwiGLU
    batch_size: 1024  # Batch size
    seq_len: 16384
    hidden_dim: 8192
    attention:
        attention_type: "mha"  # Supported attention types: "mha" or "gqa" ("mla" not implemented)
        num_heads: 64
        kv_heads: 8  # set only when attention_type is "gqa"
        use_flashattention: false  # Whether to use flash attention
        attention_tile_size: 256  # Tile size for attention computation
    num_experts: 1   # Number of experts in MoE layers. Set to 1 to disable MoE.
    top_k: 1   # Top-k experts to use in MoE layers
    ffn_dim: 32768 # First FFN layer output dimension, typically 4 * D used when ffn_mult is NULL.
    ffn_mult: null # Multiplier for FFN dimension  This is prioritized over ffn_dim.
    vocab_size: 800000  # Vocabulary size
    num_layers: 2
