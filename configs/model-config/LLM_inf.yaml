model_param:
    mode: "LLM"  # Mode of operation
    run_type: "inference" # Type of run: "training" or "inference"
    tied_embeddings: true  # Share input and output embedding weights (WIP: affects memory estimation only)
    model_type: "gpt"  # gpt assumes dense-style architecture with GELU; llama assumes llama-style architecture with SwiGLU
    global_batch_size: 2048  # Sequences per global batch
    gradient_accumulation_steps: 1  # Number of gradient accumulation micro-steps per optimizer update
    seq_len: 2048  # Total context length (prefill + decode_len for inference)
    decode_len: 1024  # Number of autoregressive decode steps.
    hidden_dim: 8192 # Transformer hidden dimension
    attention: # Attention-related parameters
        attention_type: "mha"  # Supported attention types: "mha" or "gqa" ("mla" not implemented)
        num_heads: 64 # Total number of attention heads
        kv_heads: 64  # Number of key/value heads (used only when attention_type = "gqa")
        use_flashattention: false # Enable FlashAttention optimization (when false, we model naive attention without causal masking or flash optimizations)
        attention_tile_size: 256  # Tile size used in attention computation
    moe:
      num_experts: 1
      top_k: 1
      moe_intermediate_size: 0
      n_shared_experts: 0
      moe_layer_freq: 0
      first_k_dense_replace: 0
    intermediate_size: 32768 # Output dimension of the first FFN layer (typically 4Ã—hidden_dim)
    vocab_size: 129280  # Vocabulary size
    num_layers: 32 # Number of transformer layers
