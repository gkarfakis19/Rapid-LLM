model_param:
  mode: LLM
  run_type: inference
  tied_embeddings: false
  model_type: glm4_moe
  global_batch_size: 16
  gradient_accumulation_steps: 1
  # seq_len: 131072
  seq_len: 33072
  decode_len: 4096
  hidden_dim: 5120
  attention:
    attention_type: gqa
    num_heads: 96
    kv_heads: 8
    head_dim: 128
    use_flashattention: true
    attention_tile_size: 128
  intermediate_size: 12288
  moe:
    num_experts: 160
    top_k: 8
    moe_intermediate_size: 1536
    n_shared_experts: 1
    moe_layer_freq: 1
    first_k_dense_replace: 3
  vocab_size: 151552
  num_layers: 92
