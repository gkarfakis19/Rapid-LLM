model_param:
  mode: "LLM"  # Mode of operation
  run_type: "training" # Type of run: "training" or "inference"
  tied_embeddings: true  # Share input and output embedding weights (WIP: affects memory estimation only)
  model_type: "llama"  # gpt assumes dense-style architecture with GELU; llama assumes llama-style architecture with SwiGLU
  batch_size: 1024  # Sequences per global batch
  seq_len: 2048  # Sequence length (tokens per sequence)
  hidden_dim: 4096 # Transformer hidden dimension
  attention: # Attention-related parameters
    attention_type: "mha"  # Supported attention types: "mha" or "gqa" ("mla" not implemented)
    num_heads: 32 # Total number of attention heads
    kv_heads: null  # Number of key/value heads (used only when attention_type = "gqa")
    use_flashattention: false  # Enable FlashAttention optimization
    attention_tile_size: null  # Tile size used in attention computation
  num_experts: 1   # Number of experts in MoE layers. Set to 1 to disable MoE.
  top_k: 1   # Number of active experts per token in MoE
  ffn_dim: 11008 # Output dimension of the first FFN layer (typically 4Ã—hidden_dim)
  ffn_mult: null # Multiplier for FFN dimension  This is prioritized over ffn_dim.
  vocab_size: 32000  # Vocabulary size
  num_layers: 32 # Number of transformer layers
