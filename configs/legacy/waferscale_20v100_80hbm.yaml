# Config that models 20 V100 GPUs in a node.
# We are ignoring NVLINK2.0 controller area for now.


sw_param:
  kernel_launch_overhead: 9e-6   #9us
  # dp_zero_stage selects ZeRO-style data-parallel sharding: 0=DDP, 1=ZeRO-1, 2=ZeRO-2, 3=ZeRO-3
  # only used if data parallelism (dp) > 1
  dp_zero_stage: 0
  precision:
    # tensor_format accepts numeric byte counts or dtype strings (fp16, bf16, fp32).
    # Take care that the number of flops defined below has been carefully tuned for FP16/BF16 formats.
    # For FP32 or other formats they need to be manually adjusted.
    # The following are equivalent: (tensor_format: 2.0, tensor_format: fp16, tensor_format: bf16).
    tensor_format: bf16  # baseline storage width for activations/gradients.
    # mixed_precision: true | false
    # -> false forces all activations/weights/gradients/optimizer states/stats to use tensor_format bytes. 
    # -> true enables AMP-style FP32 accumulations/optimizer states depending on param_storage_mode.
    # if true, optimizer states and stats are always FP32.
    mixed_precision: false
    # param_storage_mode: as_tensor_format | tensor_plus_fp32_master | fp32_params.
    # -> as_tensor_format makes weights/gradients use tensor_format bytes. 
    # -> tensor_plus_fp32_master makes weights/gradients use tensor_format bytes and
    #    keeps a separate FP32 master copy for weights.
    # -> fp32_params makes weights *and* gradients use FP32 bytes without a master copy.
    param_storage_mode: as_tensor_format
    # kv_cache precision may be set as bytes, dtype string, or "as_tensor_format".
    kv_cache: as_tensor_format
  h2d_bandwidth: 13297674240    # Bytes/s; set -1 to disable H2D timing 

tech_param:
  core:
    nominal_power_per_mcu: 0.13     #W
    nominal_flop_rate_per_mcu: 128
    nominal_energy_per_flop: 1.8e-13
    nominal_area_per_mcu: 0.38     #mm^2
    nominal_frequency: 1.33e9     #Hz
    nominal_voltage: 0.8
    threshold_voltage: 0.2
    margin_voltage: 0.35
    operating_area_per_mcu: 0.38     #!!!!operating_area n nominal_area should chnage hand-in-hand as they affect area scaling
    num_mcu_per_bundle: 8     #a bundle can be thought of as an SM, number of tensorcores per SM
    FMA_d1: 4                # tensor core can compute the results for an d1 x d2 x d1 mixed-precision matrix multiplication per clock
    FMA_d2: 4
    dataflow: best              #{wst. ast, ost, best, none}: wst: weight stationary, ast: activation stationary, ost: output stationary
    util: 0.85     #util should be 0.75 (12/16) for tensorcore
  DRAM:
    dynamic_energy_per_bit: 4.2e-12     #Joules
    static_power_per_bit: 0.6e-12
    area_per_bit: 11e-10     #mm2
    stack_capacity: 4 GB     # stack is the entire HBM chip!!!
    area_per_stack: 100     #mm2 # footprint of the entire chip!!! Also used to compute no. of DRAM stacks!!!!
    latency: 100e-9
    mem_ctrl_area: 5     #mm2 ## IMPORTANT: This is used to compute no. of DRAM stacks!!!!
    nominal_frequency: 1.38e9
    nominal_voltage: 1.2
    threshold_voltage: 0.4
    margin_voltage: 0.6
    max_voltage: 1.8
    num_links_per_mm: 400
    num_links_per_stack: 1024
    util: 1
  SRAM-L2:
    dynamic_energy_per_bit: 130e-15
    static_power_per_bit: 8.4e-12
    area_per_bit: 6.4e-8     #mm2
    bank_capacity: 32 KB
    controller_area_per_link: 0.004     #mm2 
    controller_power_per_link: 0.04     #W
    latency: 0
    overhead: 0.20     #20% circuitry overhead in cell area
    util: 0.45
  SRAM-L1:
    dynamic_energy_per_bit: 130e-15
    static_power_per_bit: 9e-12
    area_per_bit: 8e-8     #mm2
    bank_capacity: 32 KB
    controller_area_per_link: 0.00     #mm2 
    controller_power_per_link: 0.0     #W
    latency: 0
    overhead: 0.20
    util: 1
  SRAM-R:
    dynamic_energy_per_bit: 110e-15
    static_power_per_bit: 9e-12
    area_per_bit: 8e-8     #mm2
    bank_capacity: 16 KB
    controller_area_per_link: 0.00     #mm2 
    controller_power_per_link: 0.0     #W
    latency: 0
    overhead: 0.25
    util: 1
  network:
        # How we arrived at the intra_node NVLINK 2.0 parameters:
        # For nominal freuqnecy which is actually bits/second, we found 50GB/s PER NVLINK.
        # However, each NVLINK is composed out of 32 WIRES (16 diff pairs, 8 per direction).
        # So, 50 / 32 = 1.56 GB/s
        # So 1.56 * 8 = 12.48 Gb/s PER NVLINK
        # For energy, we found 8pj/bit.
        # nominal_enery_per_link is ACTUALLY joules/byte per link.
        # THIS MEANS WE ARE TREATING LINKS IN THE CODE AS PHYSICAL WIRES.
        # then, for num_links_per_mm, we assume 815mm^2 V100 is square.
        # perimeter is then 114.2
        # 32 physical wires per NVlink, 6 NVlinks per chip, so 32*6 = 192 links per 114 mm^2
        # Then, we DOUBLE the number to account for perimeter fraction is 0.5

        # There is a major issue. The numbers calculated above are PER GPU.
        # We have 20 workers in the node, but the code assumes the numbers are PER NODE, not per GPU. However it asks for links per ONE GPUs perimeter. THis makes no sense.
    intra_node:     # We should model NVLINK 2.0
      latency: 5e-6
      nominal_frequency: 12.48e9     # THIS IS BITS/SECOND PER LINK!!!! Look at lines 502 in hw_component.py, 678 in perf.py
      nominal_voltage: 1
          # energy per link 
      energy_per_bit: 8e-12     # This is probably pessimistic, as it is likely stated for 300mm. Our avg distance is 150mm. 8pj/Bit based on https://www.techpowerup.com/forums/threads/nvidia-is-preparing-co-packaged-photonics-for-nvlink.276139/#:~:text=While%20the%20current%20NVLink%202.0,(4%20pJ%2Fb), https://www.techpowerup.com/img/fpXXsSk543ZUFWcx_thm.jpg
      nominal_area_per_link: 1e-12     #mm^2. We are setting this to an extremely low value on purpose to ignore mem ctrl area. This is basically 0 but we cant set to 0 because of div errors.
      num_links_per_mm: 3.363     #increased a tiny bit to account for integer division and the fact that I dropped digits
      threshold_voltage: 0.25
      margin_voltage: 0.45
      util: 0.95
    inter_node:     # ignore for now, all comms are intra comms.
      latency: 5e-6
      nominal_frequency: 20e9
      nominal_voltage: 1.2
      energy_per_bit: 13e-12
      nominal_area_per_link: 0.045
      num_links_per_mm: 20     #Assuming 4 layers on PCB and 200 um pitch for high speed signalling
      threshold_voltage: 0.35
      margin_voltage: 0.5
      util: 0.96
area_breakdown: # No longer fractions. Absolute values in mm^2.
  device_area_budget: 1230   #mm^2 # DRAM Area is device_area_budget - proc_chip_area_budget. This is how we compute DRAM stack no.!
  proc_chip_area_budget: 815   #mm^2
  core: 244.5
  L2: 65.7053
  L1: 8.3945
  reg_mem: 17.93
  DRAM: 20.212   # This is the Memory controller area of the compute chip. IMPORTANT: Used to compute no. of DRAM stacks!
  network:
    intra_node: 0.0   # This is the intra-node I/O controller area of the compute chip.
    inter_node: 81.5   # This is the inter-node I/O controller area of the compute chip.

power_breakdown: # Absolute values in W. TDP is total power.
  TDP: 300
  core: 112.35
  DRAM: 48
  L2: 2.4
  L1: 31.23
  reg_mem: 90.66
  network:
    intra_node: 18.75
    inter_node: 0.0


perimeter_breakdown: # out of the compute die, what fraction of the perimeter is dedicated to IO links to each component
  DRAM: 0.5
  intra_node: 0.0
  inter_node: 0.5

system_hierarchy:
  num_devices_per_node: 1   # devices per package essentially
  num_nodes: 20   # one package
  intra_derate: 0   # derate is just a scaling down factor for the throughput of the links
  inter_derate: 1
  kp1_inter: true
  kp2_inter: true
  dp_inter: false
  lp_inter: false
  tp_inter: true

network_topology:
  inter_node: none     #mesh, torus, crossbar, custom, hierarchical
  intra_node: mesh

memory_hierarchy:
  l0:   #Register Memory
    type: SRAM-R
    scope: mcu
  l1:   #Shared Memory
    type: SRAM-L1
    scope: mcu-bundle
  l2:   #L2 
    type: SRAM-L2
    scope: global
  l3:   #Global Memory
    type: DRAM
    scope: global

scheduling_param:
  auto: false
  dp: 1
  lp: 1
  mb: 1
  tp: 20
  tp_sp: false
execution_backend:
  model: astra # analytical | astra
  astra:
    backend: astra
    mode: hybrid  # hybrid | full_astrasim_hierarchical | full_astrasim_flattened
    collective_override:
      # Allowed values per op: auto | ring | direct | halvingDoubling | doubleBinaryTree
      # Note: leave to auto unless you have a specific reason to change it.
      all_gather: auto
      all_reduce: auto
      reduce_scatter: auto
      all_to_all: auto
    sys_options:
      endpoint_delay: 10
      active_chunks_per_dimension: 32 # Higher value means significantly slower astrasim runtime, but more accurate especially for tensor parallelism
      preferred_dataset_splits: 32 # Should be same as active_chunks_per_dimension

inference:
  kvcache_type: hbm_only
