sw_param:
  full_recomputation: false # Enable full activation recompute during backward instead of selective recomputation
  kernel_launch_overhead: 9e-6   # (s)
  # dp_zero_stage selects ZeRO-style data-parallel sharding: 0=DDP, 1=ZeRO-1, 2=ZeRO-2, 3=ZeRO-3
  # only used if data parallelism (dp) > 1
  dp_zero_stage: 2
  precision:
    # tensor_format accepts numeric byte counts or dtype strings (fp16, bf16, fp32).
    # Take care that the number of flops defined below has been carefully tuned for FP16/BF16 formats.
    # For FP32 or other formats they need to be manually adjusted.
    # The following are equivalent: (tensor_format: 2.0, tensor_format: fp16, tensor_format: bf16).
    tensor_format: bf16  # baseline storage width for activations/gradients.
    # mixed_precision: true | false
    # -> false forces all activations/weights/gradients/optimizer states/stats to use tensor_format bytes.
    # -> true enables AMP-style FP32 accumulations/optimizer states depending on param_storage_mode.
    # if true, optimizer states and stats are always FP32.
    mixed_precision: false
    # param_storage_mode: as_tensor_format | tensor_plus_fp32_master | fp32_params.
    # -> as_tensor_format makes weights/gradients use tensor_format bytes.
    # -> tensor_plus_fp32_master makes weights/gradients use tensor_format bytes and
    #    keeps a separate FP32 master copy for weights.
    # -> fp32_params makes weights *and* gradients use FP32 bytes without a master copy.
    param_storage_mode: as_tensor_format
    # kv_cache precision may be set as bytes, dtype string, or "as_tensor_format".
    kv_cache: as_tensor_format
  h2d_bandwidth: -1  # Bytes/s; set -1 to disable H2D timing
tech_param:
  core:
    operating_frequency: 1.41e9     # (Hz)
    num_bundles: 108
    nominal_flop_rate_per_mcu: 512
    nominal_energy_per_flop: 1.8e-13 # (J)
    num_mcu_per_bundle: 4     # number of tensorcores per SM
    FMA_d1: 8                 # tensor core can compute the results for an d1 x d2 x d1 mixed-precision matrix multiplication per clock
    FMA_d2: 4
    dataflow: best            # {wst | ast | ost | best}: wst: weight stationary, ast: activation stationary, ost: output stationary, best: best of all 3
    util: 100.0
  DRAM:
    size: 80 GB
    bandwidth: 198600 GB     # (/s)
    dynamic_energy_per_bit: 4.5e-12     # (J)
    latency: 100e-9     # (s)
    util: 1
  SRAM-L2:
    size: 40 MB
    bandwidth: 7050 GB
    dynamic_energy_per_bit: 130e-15 # (J)
    latency: 0
    util: 1
  SRAM-L1:
    size: 20736 KB
    bandwidth: 18 TB
    dynamic_energy_per_bit: 130e-15 # (J)
    latency: 0
    util: 1
  SRAM-R:
    size: 27 MB
    bandwidth: 122 TB     
    dynamic_energy_per_bit: 110e-15 # (J)
    latency: 0
    util: 1

memory_hierarchy:
  l0:   #Register Memory
    type: SRAM-R
    scope: mcu
  l1:   #Shared Memory
    type: SRAM-L1
    scope: mcu-bundle
  l2:   #L2
    type: SRAM-L2
    scope: global
  l3:   #Global Memory
    type: DRAM
    scope: global

parallelism:
  dp: 2 # data parallelism. Acts as replica count during inference
  tp: 3 # tensor parallelism
  tp_sp: true # tensor-sequence parallelism
  cp: 2 # context parallelism. Not supported for inference.
  lp: 3 # pipeline parallelism. Number of pipeline stages
  # mb is not a true parallelism parameter, does not affect num of GPUs.
  mb: 2 # micro-batches (for best results set to >= lp). 

network:
  # currently only support astrasim integration with all devices in one node.
  - id: dim0
    label: intra_wafer_mesh_silicon
    size: 6
    topology:
      type: Ring
      bandwidth: 1000 Gb
      latency: 5e-6
      energy_per_bit: 8e-12 # (J/bit)
      util: 1.0
    collective_override: {} # overrides execution_backend.collectives when provided
    parallelisms: [tp, cp]
  - id: dim1
    label: inter_wafer_ring_infiniband
    size: 3
    topology:
      type: Ring
      bandwidth: 200 Gb
      latency: 5e-6
      energy_per_bit: 8e-12 # (J/bit)
      util: 1.0
    collective_override: {} # overrides execution_backend.collectives when provided
    parallelisms: [lp]
  - id: dim2
    label: wafer_dp_chains_ring_infiniband
    size: 2
    topology:
      type: ring
      bandwidth: 200 Gb
      latency: 5e-6
      energy_per_bit: 8e-12 # (J/bit)
      util: 0.96
    collective_override: {} # overrides execution_backend.collectives when provided
    parallelisms: [dp]


execution_backend:
  model: astra # analytical | astra
  astra:
    backend: astra
    mode: full_astrasim_flattened  # hybrid | full_astrasim_hierarchical | full_astrasim_flattened
    collectives:
      # Default collective algorithms. Per-dimension collective_override entries can override these.
      # Allowed values per op: auto | ring | direct | halvingDoubling | doubleBinaryTree | mesh | mesh2d | hypercube | torus2d
      # Note: leave to auto unless you have a specific reason to change it.
      all_gather: auto
      all_reduce: auto
      reduce_scatter: auto
      all_to_all: auto
    sys_options:
      endpoint_delay: 5
      active_chunks_per_dimension: 32 # Higher value means significantly slower astrasim runtime, but more accurate especially for tensor parallelism
      preferred_dataset_splits: 4 # Should be same as active_chunks_per_dimension

inference:
  kvcache_type: hbm_only # Only supported mode for now.
