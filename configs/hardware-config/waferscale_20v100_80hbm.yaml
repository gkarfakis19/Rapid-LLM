# Config that models 20 V100 GPUs in a node.
# We are ignoring NVLINK2.0 controller area for now.


sw_param:
  full_recomputation: false # Enable full activation recompute during backward instead of selective recomputation
  kernel_launch_overhead: 9e-6   #9us
  # dp_zero_stage selects ZeRO-style data-parallel sharding: 0=DDP, 1=ZeRO-1, 2=ZeRO-2, 3=ZeRO-3
  # only used if data parallelism (dp) > 1
  dp_zero_stage: 0
  precision:
    # tensor_format accepts numeric byte counts or dtype strings (fp16, bf16, fp32).
    # Take care that the number of flops defined below has been carefully tuned for FP16/BF16 formats.
    # For FP32 or other formats they need to be manually adjusted.
    # The following are equivalent: (tensor_format: 2.0, tensor_format: fp16, tensor_format: bf16).
    tensor_format: bf16  # baseline storage width for activations/gradients.
    # mixed_precision: true | false
    # -> false forces all activations/weights/gradients/optimizer states/stats to use tensor_format bytes.
    # -> true enables AMP-style FP32 accumulations/optimizer states depending on param_storage_mode.
    # if true, optimizer states and stats are always FP32.
    mixed_precision: false
    # param_storage_mode: as_tensor_format | tensor_plus_fp32_master | fp32_params.
    # -> as_tensor_format makes weights/gradients use tensor_format bytes.
    # -> tensor_plus_fp32_master makes weights/gradients use tensor_format bytes and
    #    keeps a separate FP32 master copy for weights.
    # -> fp32_params makes weights *and* gradients use FP32 bytes without a master copy.
    param_storage_mode: as_tensor_format
    # kv_cache precision may be set as bytes, dtype string, or "as_tensor_format".
    kv_cache: as_tensor_format
  h2d_bandwidth: 13297674240    # Bytes/s; set -1 to disable H2D timing

tech_param:
  core:
    operating_frequency: 1.33e9     #Hz
    num_bundles: 1600     # 80 SMs per V100 * 20 GPUs
    nominal_flop_rate_per_mcu: 128
    nominal_energy_per_flop: 1.8e-13
    num_mcu_per_bundle: 8     #a bundle can be thought of as an SM, number of tensorcores per SM
    FMA_d1: 4                # tensor core can compute the results for an d1 x d2 x d1 mixed-precision matrix multiplication per clock
    FMA_d2: 4
    dataflow: best              #{wst. ast, ost, best, none}: wst: weight stationary, ast: activation stationary, ost: output stationary
    util: 0.85     #util should be 0.75 (12/16) for tensorcore
  DRAM:
    size: 320 GB     # 16 GB per V100 * 20 GPUs
    bandwidth: 18000 GB     # ~900 GB/s per V100 * 20
    dynamic_energy_per_bit: 4.2e-12     #Joules
    latency: 100e-9
    util: 1
  SRAM-L2:
    size: 120 MB     # 6 MB per V100 * 20
    bandwidth: 61440 GB     # Estimated L2 bandwidth
    dynamic_energy_per_bit: 130e-15
    latency: 0
    util: 0.45
  SRAM-L1:
    size: 128000 KB     # 6400 KB per V100 * 20
    bandwidth: 300 TB     # Estimated L1 bandwidth
    dynamic_energy_per_bit: 130e-15
    latency: 0
    util: 1
  SRAM-R:
    size: 400 MB     # Register file
    bandwidth: 1600 TB     # Estimated register bandwidth
    dynamic_energy_per_bit: 110e-15
    latency: 0
    util: 1
  network:
        # How we arrived at the intra_node NVLINK 2.0 parameters:
        # For nominal freuqnecy which is actually bits/second, we found 50GB/s PER NVLINK.
        # However, each NVLINK is composed out of 32 WIRES (16 diff pairs, 8 per direction).
        # So, 50 / 32 = 1.56 GB/s
        # So 1.56 * 8 = 12.48 Gb/s PER NVLINK
        # For energy, we found 8pj/bit.
        # nominal_enery_per_link is ACTUALLY joules/byte per link.
        # THIS MEANS WE ARE TREATING LINKS IN THE CODE AS PHYSICAL WIRES.
        # then, for num_links_per_mm, we assume 815mm^2 V100 is square.
        # perimeter is then 114.2
        # 32 physical wires per NVlink, 6 NVlinks per chip, so 32*6 = 192 links per 114 mm^2
        # Then, we DOUBLE the number to account for perimeter fraction is 0.5

        # There is a major issue. The numbers calculated above are PER GPU.
        # We have 20 workers in the node, but the code assumes the numbers are PER NODE, not per GPU. However it asks for links per ONE GPUs perimeter. THis makes no sense.
    intra_node:     # We should model NVLINK 2.0
      latency: 5e-6
      util: 0.95
      bandwidth: 5000 Gb  # Total NVLINK bandwidth for 20 GPUs
    inter_node:     # ignore for now, all comms are intra comms.
      latency: 5e-6
      util: 0.96
      bandwidth: 300 Gb

memory_hierarchy:
  l0:   #Register Memory
    type: SRAM-R
    scope: mcu
  l1:   #Shared Memory
    type: SRAM-L1
    scope: mcu-bundle
  l2:   #L2
    type: SRAM-L2
    scope: global
  l3:   #Global Memory
    type: DRAM
    scope: global

parallelism:
  auto: false
  dp: 1
  lp: 1
  mb: 1
  tp: 20
  tp_sp: false
  cp: 1

network:
  dimensions:
    - id: dim0
      label: wafer_mesh
      size: 20
      topology:
        type: Mesh
        bandwidth: 5000 Gb
        latency: 5e-6
        energy_per_bit: 8e-12 # (J/bit)
        util: 0.95
      collective_override: {} # overrides execution_backend.collectives when provided
      parallelisms: [tp, cp, lp, dp]

execution_backend:
  model: astra # analytical | astra
  astra:
    backend: astra
    mode: hybrid  # hybrid | full_astrasim_hierarchical | full_astrasim_flattened
    collectives:
      # Default collective algorithms. Per-dimension collective_override entries can override these.
      # Allowed values per op: auto | ring | direct | halvingDoubling | doubleBinaryTree | mesh | mesh2d | hypercube | torus2d
      # Note: leave to auto unless you have a specific reason to change it.
      all_gather: auto
      all_reduce: auto
      reduce_scatter: auto
      all_to_all: auto
    sys_options:
      endpoint_delay: 10
      active_chunks_per_dimension: 32 # Higher value means significantly slower astrasim runtime, but more accurate especially for tensor parallelism
      preferred_dataset_splits: 32 # Should be same as active_chunks_per_dimension

inference:
  kvcache_type: hbm_only
