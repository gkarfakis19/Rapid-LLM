# RAPID-LLM configuration for NVIDIA H100 PCIe 80 GB
# specifications from the following sources:
# [NVIDIA] https://resources.nvidia.com/en-us-hopper-architecture/nvidia-h100-tensor-c
# [TePU] https://www.techpowerup.com/gpu-specs/h100-sxm5-80-gb.c3900
# [Semi] https://semianalysis.com/2025/06/23/nvidia-tensor-core-evolution-from-volta-to-blackwell/#4th-generation-tensor-core-warpgroup-level-asynchronous-mma

sw_param:
  full_recomputation: false # Enable full activation recompute during backward instead of selective recomputation
  kernel_launch_overhead: 9e-6   # (s)
  # dp_zero_stage selects ZeRO-style data-parallel sharding: 0=DDP, 1=ZeRO-1, 2=ZeRO-2, 3=ZeRO-3
  # only used if data parallelism (dp) > 1
  dp_zero_stage: 1
  precision:
    # tensor_format accepts numeric byte counts or dtype strings (fp16, bf16, fp32).
    # Take care that the number of flops defined below has been carefully tuned for FP16/BF16 formats.
    # For FP32 or other formats they need to be manually adjusted.
    # The following are equivalent: (tensor_format: 2.0, tensor_format: fp16, tensor_format: bf16).
    tensor_format: bf16  # baseline storage width for activations/gradients.
    # kv_cache precision may be set as bytes, dtype string, or "as_tensor_format".
    kv_cache: as_tensor_format
    # Model parameter precision
    parameters: as_tensor_format
    # Accumulated gradients (persistent FP32 buffer)
    gradients: fp32
    # Ephemeral per-microbatch gradients
    grad_microbatch: as_tensor_format
    # Gradient communication precision
    grad_communication: as_tensor_format
    # Optimizer states (momentum, variance)
    optimizer_states: fp32
    # Training statistics
    stats: fp32
    # FP32 master weight copy (0 = disabled)
    master_parameters: 0.0
  h2d_bandwidth: -1  # Bytes/s; set -1 to disable H2D timing
  # DP gradient sync frequency: every_mb (default) or last_mb (sync only on final microbatch; ZeRO-3 stays per-mb)
  dp_microbatch: last_mb
tech_param:
  core:
    operating_frequency: 1.98e9     # (Hz) GPU boost clock frequency [TePU]
    num_bundles: 132     # number of bundles (SMs) [TePU]
    nominal_flop_rate_per_mcu: 1024     # F16 FLOP / cycle / SM is 4096 [Semi]
    nominal_energy_per_flop: 1.8e-13
    num_mcu_per_bundle: 4     # number of tensorcores per SM [TePU]
    FMA_d1: 16                # tensor core can compute the results for an d1 x d2 x d1 mixed-precision matrix multiplication per clock
    FMA_d2: 8
    dataflow: best
    util: 1.0
  DRAM:
    size: 80 GB     # [TePU]
    bandwidth: 3430 GB     # (/s) [TePU]
    dynamic_energy_per_bit: 4.5e-12     # (J) [https://passlab.github.io/mchpc/mchpc2019/presentations/MCHPC_Pawlowski_keynote.pdf, slide 36]
    latency: 100e-9     # (s)
    util: 1
  SRAM-L2:
    size: 50 MB     # [TePU]
    bandwidth: 8138 GB     # NCU L2 peak traffic
    dynamic_energy_per_bit: 130e-15
    latency: 0
    util: 1
  SRAM-L1:
    size: 30096 KB     # max configurable shared memory per SM * number of SMs [NVIDIA]
    bandwidth: 22392 GB     # NCU L1/TEX peak traffic
    dynamic_energy_per_bit: 130e-15
    latency: 0
    util: 1
  SRAM-R:
    size: 33 MB     # register file memory per SM * number of SMs [NVIDIA]
    bandwidth: 122 TB     # traffic for 2 reads and 1 write for GEMMs computed per clock
    dynamic_energy_per_bit: 110e-15
    latency: 0
    util: 1

memory_hierarchy:
  l0:   #Register Memory
    type: SRAM-R
    scope: mcu
  l1:   #Shared Memory
    type: SRAM-L1
    scope: mcu-bundle
  l2:   #L2
    type: SRAM-L2
    scope: global
  l3:   #Global Memory
    type: DRAM
    scope: global

parallelism:
  auto: false
  ### LLM PARAMS ###
  tp: 8 # tensor parallelism
  tp_sp: false # tensor-sequence parallelism
  cp: 1 # context parallelism. Not supported for inference.
  lp: 1 # pipeline parallelism. Number of pipeline stages
  # mb is not a true parallelism parameter, does not affect num of GPUs.
  mb: 1 # micro-batches (for best results set to >= lp). 

  train:
    dp: 1 # data parallelism
    ep: 1 # expert parallelism
    # controls expert TP sharding: if false, tp and ep are applied separately to attn/FFN; if true, TP applies to both
    tp_ep: true
  inference:
    replica_count: 1 # inference throughput scaling only
    moe_dp: 1 # inference expert pool expansion (EP_eff = tp * moe_dp)
network:
  dimensions:
    - id: dim0
      label: torus2d_tp
      size: auto
      topology:
        type: Ring
        bandwidth: 100 GB # We assume 8 25GB/s NVLINKs in the torus. Torus2D is hardcoded bidirectional, so it's actually 4 links, 25GB/s each (because they get replicated). So 100GB/s total. 
        latency: 5e-6
        energy_per_bit: 8e-12 # (J/bit)
        util: 1.0
        optimize_2dmap: false
      collective_override: {} # overrides execution_backend.collectives when provided
      parallelisms: [tp, cp]
    - id: dim1
      label: inter_torus_infiniband
      size: auto
      topology:
        type: Ring 
        bandwidth: 25 GB
        latency: 5e-6
        energy_per_bit: 8e-12 # (J/bit)
        util: 1.0
      collective_override: {} # overrides execution_backend.collectives when provided
      parallelisms: [lp,dp]
    - id: dim2
      label: inter_torus_infiniband_dp
      size: auto
      topology:
        type: Ring
        bandwidth: 25 GB
        latency: 5e-6
        energy_per_bit: 8e-12 # (J/bit)
        util: 1.0
      parallelisms: []
      collective_override: {} # overrides execution_backend.collectives when provided

  overlap:
    # fraction of TP+SP compute that can overlap TP/SP comm
    tp_sp_overlap: 0.63
    # fraction of TP compute that can overlap TP comm
    tp_overlap: 0.6
    # fraction of CP comm that can overlap CP compute
    cp_overlap: 0.0
execution_backend:
  model: analytical # analytical | astra
  astra:
    backend: astra
    mode: full_astrasim_flattened  # hybrid | full_astrasim_hierarchical | full_astrasim_flattened
    collectives:
      # Default collective algorithms. Per-dimension collective_override entries can override these.
      # Allowed values per op: auto | ring | direct | halvingDoubling | doubleBinaryTree | mesh | mesh2d | hypercube | torus2d
      # Note: leave to auto unless you have a specific reason to change it.
      all_gather: auto
      all_reduce: auto
      reduce_scatter: auto
      all_to_all: auto
    sys_options:
      endpoint_delay: 5
      active_chunks_per_dimension: 32 # Higher value means significantly slower astrasim runtime, but more accurate especially for tensor parallelism
      preferred_dataset_splits: 4 # Should be same as active_chunks_per_dimension

inference:
  kvcache_type: hbm_only
